{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_IMPORTANT_! Make sure you have previously accepted the data terms on the kaggle website by downloading one of the competition's data files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scripts in kaggle directory\n",
    "import sys\n",
    "sys.path.append('/vagrant/kaggle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Download data files.\n",
    "from crowdflower import CrowdFlower\n",
    "from zipfile import ZipFile\n",
    "files = CrowdFlower.get_data()\n",
    "_ = [ZipFile(z).extractall(CrowdFlower.__data_path__) for z in CrowdFlower.get_data() if z[-4:] == '.zip']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir(CrowdFlower.__data_path__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(CrowdFlower.__data_path__ + \"/train.csv\").fillna(\"\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test  = pd.read_csv(CrowdFlower.__data_path__ + \"/test.csv\").fillna(\"\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FeatureMapper:\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            extractor.fit(X[column_name], y)\n",
    "\n",
    "    def transform(self, X):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            fea = extractor.transform(X[column_name])\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        extracted = []\n",
    "        for feature_name, column_name, extractor in self.features:\n",
    "            fea = extractor.fit_transform(X[column_name], y)\n",
    "            if hasattr(fea, \"toarray\"):\n",
    "                extracted.append(fea.toarray())\n",
    "            else:\n",
    "                extracted.append(fea)\n",
    "        if len(extracted) > 1:\n",
    "            return np.concatenate(extracted, axis=1)\n",
    "        else: \n",
    "            return extracted[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class SimpleTransform(BaseEstimator):\n",
    "    def __init__(self, transformer=identity):\n",
    "        self.transformer = transformer\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "        return self.transform(X)\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return np.array([self.transformer(x) for x in X], ndmin=2).T\n",
    "\n",
    "#                          Feature Set Name            Data Frame Column              Transformer\n",
    "features = FeatureMapper([('QueryBagOfWords',          'query',                       CountVectorizer(max_features=200)),\n",
    "                          ('TitleBagOfWords',          'product_title',               CountVectorizer(max_features=200)),\n",
    "                          ('DescriptionBagOfWords',    'product_description',         CountVectorizer(max_features=200)),\n",
    "                          ('QueryTokensInTitle',       'query_tokens_in_title',       SimpleTransform()),\n",
    "                          ('QueryTokensInDescription', 'query_tokens_in_description', SimpleTransform())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(data):\n",
    "    token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "    data[\"query_tokens_in_title\"] = 0.0\n",
    "    data[\"query_tokens_in_description\"] = 0.0\n",
    "    for i, row in data.iterrows():\n",
    "        query = set(x.lower() for x in token_pattern.findall(row[\"query\"]))\n",
    "        title = set(x.lower() for x in token_pattern.findall(row[\"product_title\"]))\n",
    "        description = set(x.lower() for x in token_pattern.findall(row[\"product_description\"]))\n",
    "        if len(title) > 0:\n",
    "            data.set_value(i, \"query_tokens_in_title\", len(query.intersection(title))/len(title))\n",
    "        if len(description) > 0:\n",
    "            data.set_value(i, \"query_tokens_in_description\", len(query.intersection(description))/len(description))\n",
    "\n",
    "extract_features(train)\n",
    "extract_features(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([(\"extract_features\", features),\n",
    "                     (\"classify\", RandomForestClassifier(n_estimators=200,\n",
    "                                                         n_jobs=-1,\n",
    "                                                         min_samples_split=2,\n",
    "                                                         random_state=1))])\n",
    "\n",
    "pipeline.fit(train, train[\"median_relevance\"])\n",
    "\n",
    "predictions = pipeline.predict(test)\n",
    "\n",
    "submission = pd.DataFrame({\"id\": test[\"id\"], \"prediction\": predictions})\n",
    "submission.to_csv(CrowdFlower.__data_path__ + \"/crowdflower_benchmark.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
